\documentclass{article}


\usepackage[]{algorithm2e}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{listings} % For displaying code

% Solutions and rubrics
\def\ans#1{\par\gre{Answer: #1}}
% \def\ans#1{} % UNCOMMENTED => NO ANSWERS ; COMMENTED => ANSWERS
\def\answer#1{\ans{#1}}
\def\rubric#1{\gre{Rubric: \{#1\}}}{}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}


% Math
\def\norm#1{\|#1\|}
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}
\def\half{\frac 1 2}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}

\begin{document}
	
	\title{CPSC 340 Assignment 1 (due 2021-05-17 at 9:25 am)}
	
	\date{}
	\maketitle
	
	\red{We are providing solutions because supervised learning is easier than unsupervised learning, and so we think having solutions available can help you learn. However, the solution file is meant for you alone and we do not give permission to share these solution files with anyone. Both distributing solution files to other people or using solution files provided to you by other people are considered academic misconduct. Please see UBC's policy on this topic if you are not familiar with it:\\
		\url{http://www.calendar.ubc.ca/vancouver/index.cfm?tree=3,54,111,959}\\
		\url{http://www.calendar.ubc.ca/vancouver/index.cfm?tree=3,54,111,960}}
	
	
	\textbf{Commentary on Assignment 1}: CPSC 340 is tough because it combines knowledge and skills across several disciplines. To succeed
	in the course, you will need to know or very quickly get up to speed on:
	\begin{itemize}
		\item Basic Python programming, including NumPy and plotting with matplotlib.
		\item Math to the level of the course prerequisites: linear algebra, multivariable calculus, some probability.
		\item Statistics, algorithms and data structures to the level of the course prerequisites.
		\item Some basic LaTeX skills so that you can typeset equations and submit your assignments.
	\end{itemize}
	
	This assignment will help you assess whether you are prepared for this course. We anticipate that each
	of you will have different strengths and weaknesses, so don't be worried if you struggle with \emph{some} aspects
	of the assignment. \textbf{But if you find this assignment
		to be very difficult overall, that is a warning sign that you may not be prepared to take CPSC 340
		at this time.} Future assignments will be more difficult than this one (and probably around the same length).
	
	Questions 1-4 are on review material, that we expect you to know coming into the course. The rest is new CPSC 340 material from the first few lectures.
	
	\textbf{A note on the provided code:} in the \texttt{code} directory we provide you with a file called
	\texttt{main.py}. This file, when run with different arguments, runs the code for different
	parts of the assignment. For example,
	\begin{verbatim}
		python main.py -q 6.2
	\end{verbatim}
	runs the code for Question 6.2. At present, this should do nothing (throws a \texttt{NotImplementedError}), because the code
	for Question 6.2 still needs to be written (by you). But we do provide some of the bits
	and pieces to save you time, so that you can focus on the machine learning aspects.
	For example, you'll see that the provided code already loads the datasets for you.
	The file \texttt{utils.py} contains some helper functions.
	You don't need to read or modify the code in there.
	To complete your assignment, you will need to modify \texttt{grads.py}, \texttt{main.py}, \texttt{decision\string_stump.py} and \texttt{simple\string_decision.py} (which you'll need to create).
	
	
	\section*{Instructions}
	\rubric{points:5}
	
	\vspace{1em}
	We use \blu{blue} to highlight the deliverables that you must answer/do/submit with the assignment.
	
	\section{Linear Algebra Review}
	
	For these questions you may find it helpful to review these notes on linear algebra:\\
	\url{http://www.cs.ubc.ca/~schmidtm/Documents/2009_Notes_LinearAlgebra.pdf}
	
	\subsection{Basic Operations}
	\rubric{points:7}
	
	Use the definitions below,
	\[
	\alpha = 2,\quad
	x = \left[\begin{array}{c}
		0\\
		1\\
		2\\
	\end{array}\right], \quad
	y = \left[\begin{array}{c}
		3\\
		4\\
		5\\
	\end{array}\right],\quad
	z = \left[\begin{array}{c}
		1\\
		2\\
		-1\\
	\end{array}\right],\quad
	A = \left[\begin{array}{ccc}
		3 & 2 & 2\\
		1 & 3 & 1\\
		1 & 1 & 3
	\end{array}\right],
	\]
	and use $x_i$ to denote element $i$ of vector $x$.
	\blu{Evaluate the following expressions} (you do not need to show your work).
	\enum{
		\item $\sum_{i=1}^n x_iy_i$ (inner product).
		\ans{$0\cdot3 + 1\cdot 4 + 2\cdot5 = 14$.}
		\item $\sum_{i=1}^n x_iz_i$ (inner product between orthogonal vectors).
		\ans{$0\cdot1 + 1\cdot2 + 2\cdot(-1) = 0$.}
		\item $\alpha(x+z)$ (vector addition and scalar multiplication)
		\ans{$2\left(\left[\begin{array}{c}
				0\\
				1\\
				2\\
			\end{array}\right] + \left[\begin{array}{c}
				1\\
				2\\
				-1\\
			\end{array}\right]\right) = 2\left[\begin{array}{c}1\\3\\1\end{array}\right] = \left[\begin{array}{c}2\\6\\2\end{array}\right]$}
		\item $x^Tz + \norm{x}$ (inner product in matrix notation and Euclidean norm of $x$).
		\ans{$0 + \sqrt{0^2 + 1^2 + 2^2} = \sqrt{5}$.}
		\item $Ax$ (matrix-vector multiplication).
		\ans{$\mat{3 & 2 & 2\\
				1 & 3 & 1\\
				1 & 1 & 3}\mat{0\\1\\2} = \mat{3\cdot0 + 2\cdot1 + 2\cdot2\\1\cdot0 + 3\cdot1 + 1\cdot2\\1\cdot0 + 1\cdot1 + 3\cdot2} = \mat{6\\5\\7}$.}
		\item $x^TAx$ (quadratic form).
		\ans{$\mat{0\\1\\2} \mat{6\\5\\7} = 0\cdot 6 + 1\cdot 5 + 2\cdot 7 = 0 + 5 + 14 = 19$.}
		\item $A^TA$ (matrix tranpose and matrix multiplication).
		\ans{$\mat{3 & 1 & 1\\
				2 & 3 & 1\\
				2 & 1 & 3}\mat{3 & 2 & 2\\
				1 & 3 & 1\\
				1 & 1 & 3} = \mat{11 & 10 & 10\\10 & 14 & 10\\10 & 10 & 14}$.}
	}
	
	\subsection{Matrix Algebra Rules}
	\rubric{points:10}
	
	Assume that $\{x,y,z\}$ are $n \times 1$ column vectors, $\{A,B,C\}$ are $n \times n$ real-valued matrices, \red{$0$ is the zero matrix of appropriate size}, and $I$ is the identity matrix of appropriate size. \blu{State whether each of the below is true in general} (you do not need to show your work).
	
	\begin{enumerate}
		\item $x^Ty = \sum_{i=1}^n x_iy_i$.
		\ans{True (by using definition of matrix transpose and matrix product).}
		\item $x^Tx = \norm{x}^2$.
		\ans{True ($\norm{x}^2 = \left(\sqrt{\sum_{i=1}^n x_i^2}\right)^2 = \sum_{i=1}^n x_i^2 = \sum_{i=1}^n x_ix_i = x^Tx$).}
		\item $x^Tx = xx^T$.
		\ans{False (the left side is a scalar and the right side is a matrix).}
		\item $(x-y)^T(x-y) = \norm{x}^2 - 2x^Ty + \norm{y}^2$.
		\ans{True ($(x-y)^T(x-y) = x^Tx - x^Ty - y^Tx + y^Ty$, while $x^Ty=y^Tx$ and $x^Tx = \norm{x}^2$).}
		\item $AB=BA$.
		\ans{False (matrix-multiply is not commutative).}
		\item $A^T(B + IC) = A^TB + A^TC$.
		\ans{True (identity matrix can be removed, matrix-multiply is distributive across addition).}
		\item $(A + BC)^T = A^T + B^TC^T$.
		\ans{False (the second term should be $C^TB^T$).}
		\item $x^TAy = y^TA^Tx$.
		\ans{True (dimension of result is $1 \times 1$, and scalar equals its transpose).}
		\item $A^TA = AA^T$ if $A$ is a symmetric matrix.
		\ans{True (symmetry implies $A = A^T$ so $A^TA = AA = AA^T$).}
		\item $A^TA = 0$ if the columns of $A$ are orthonormal.
		\ans{False (it should be $A^TA = I$).}
	\end{enumerate}
	
	\section{Probability Review}
	
	
	For these questions you may find it helpful to review these notes on probability:\\
	\url{http://www.cs.ubc.ca/~schmidtm/Courses/Notes/probability.pdf}\\
	And here are some slides giving visual representations of the ideas as well as some simple examples:\\
	\url{http://www.cs.ubc.ca/~schmidtm/Courses/Notes/probabilitySlides.pdf}
	
	\subsection{Rules of probability}
	\rubric{points:6}
	
	\blu{Answer the following questions.} You do not need to show your work.
	
	% Combinations/enumeration questions, independence question
	
	\begin{enumerate}
		\item You are offered the opportunity to play the following game: your opponent rolls 2 regular 6-sided dice. If the difference between the two rolls is at least 3, you win \$15. Otherwise, you get nothing. What is a fair price for a ticket to play this game once? In other words, what is the expected value of playing the game?
		\ans{There are $12$ ways this can happen (the sets $(3,6), (2,5), (2,6), (1,4), (1,5), (1,6)$ and their reverses) out of $36$ possibilities, giving $\frac{12}{36}15 + \frac{24}{36}0 = 5$.}
		\item Consider two events $A$ and $B$ such that $\Pr(A, B)=0$ (they are mutually exclusive). If $\Pr(A) = 0.4$ and $\Pr(A \cup B) = 0.95$, what is $\Pr(B)$? Note: $p(A, B)$ means
		``probability of $A$ and $B$'' while $p(A \cup B)$ means ``probability of $A$ or $B$''. It may be helpful to draw a Venn diagram.
		\ans{$p(A \cup B) = p(A) + p(B) - p(A,B)$ so $0.95 = 0.4 + p(B)$ or $p(B) = 0.55$.}
		\item Instead of assuming that $A$ and $B$ are mutually exclusive ($\Pr(A,B) = 0)$, what is the answer to the previous question if we assume that $A$ and $B$ are independent?
		\ans{Independence means that $p(A, B) = p(A)p(B)$. So we need to solve the linear equation $0.95 = 0.4 + p(B)  - 0.4\cdot p(B)$. This is equivalent to $(1-.4)p(B) = 0.55$, or $p(B) = \frac{0.55}{0.6} = 0.91\overline{6}$ or $11/12$.}
		
		
	\end{enumerate}
	
	\subsection{Bayes Rule and Conditional Probability}
	\rubric{points:10}
	
	\blu{Answer the following questions.} You do not need to show your work.
	
	Suppose a drug test produces a positive result with probability $0.97$ for drug users, $P(T=1 \mid D=1)=0.97$. It also produces a negative result with probability $0.99$ for non-drug users, $P(T=0 \mid D=0)=0.99$. The probability that a random person uses the drug is $0.0001$, so $P(D=1)=0.0001$.
	
	\begin{enumerate}
		\item What is the probability that a random person would test positive, $P(T=1)$?
		\ans{\alignStar{
				p(T = 1) & = p(T = 1 \mid D = 1)p(D = 1) + p(T = 1 \mid D = 0)p(D = 0)\\
				& = (0.97)(0.0001) + (1-0.99)(1-0.0001)\\
				& \approx 0.000097 + 0.009999\\
				& \approx 0.010096.
		}}
		\item In the above, do most of these positive tests come from true positives or from false positives?
		\ans{Most come from false positives ($\approx 0.01$ vs. $\approx 0.0001$).}
		\item What is the probability that a random person who tests positive is a user, $P(D=1 \mid T=1)$?
		\ans{\alignStar{
				p(D = 1 \mid T = 1) & = \frac{p(T = 1 \mid D=1)p(D=1)}{p(T = 1)}\\
				& = \frac{(0.97)(0.0001)}{(0.97)(0.0001) + (1-0.99)(1-0.0001)}\\
				& \approx 0.0096.
		}}
		\item Suppose you have given this test to a random person and it came back positive, are they likely to be a drug user?
		\ans{No, only about $1\%$ of positives tests are drug users. The vast majority of positive tests are not going to be drug users (false positives).}
		\item Suppose you are the designer of this drug test. You may change how the test is conducted, which may influence factors like false positive rate, false negative rate, and the number of samples collected. What is one factor you could change to make this a more useful test?
		\ans{Increasing $p(T=1 \mid D=1)$ even to $1$ doesn't help much. You would need to significantly decrease the probability of the test giving false positives, $p(T = 1 \mid D = 0)$. Alternately, it would be useful if you changed the sampling popluation to increase $p(D = 1)$ (maybe first giving another test with a much lower probability of false positives).}
	\end{enumerate}
	
	
	\section{Calculus Review}
	
	
	
	\subsection{One-variable derivatives}
	\label{sub.one.var}
	\rubric{points:8}
	
	\blu{Answer the following questions.} You do not need to show your work.
	
	\begin{enumerate}
		\item Find the derivative of the function $f(x) = 3x^2 -2x + 5$.
		\ans{$f'(x) = 6x - 2$.}
		\item Find the derivative of the function $f(x) = x(1-x)$.
		\ans{$f'(x) = 1 - 2x$}
		\item Let $p(x) = \frac{1}{1+\exp(-x)}$ for $x \in \R$. Compute the derivative of the function $f(x) = x-\log(p(x))$ and simplify it by using the function $p(x)$.
		\ans{Using that $\log(1) = 0$ and the log rules for division we get $f(x) = x -\log(1)+\log(1+\exp(-x)) = x + \log(1+\exp(-x))$. Applying the chain rule and the definition of of $p$ gives
			\alignStar{
				f'(x) & = 1-\frac{\exp(-x)}{1+\exp(-x)}\\
				& = 1-\frac{1}{1+\exp(x)}\\
				& = 1-(1-p(x))\\
				& = p(x).
			}
		}
	\end{enumerate}
	Remember that in this course we will $\log(x)$ to mean the ``natural'' logarithm of $x$, so that $\log(\exp(1)) = 1$. Also, obseve that $p(x) = 1-p(-x)$ for the final part.
	
	\subsection{Multi-variable derivatives}
	\label{sub.multi.var}
	\rubric{points:5}
	
	\blu{Compute the gradient vector $\nabla f(x)$ of each of the following functions.} You do not need to show your work.
	\begin{enumerate}
		\item $f(x) = x_1^2 + \exp(x_1 + 2x_2)$ where $x \in \R^2$.
		\ans{$\nabla f(x) = \mat{2x_1 + \exp(x_1+2x_2)\\ 2\exp(x_1 + 2x_2)}$.}
		\item $f(x) = \log\left(\sum_{i=1}^3\exp(x_i)\right)$ where $x \in \R^3$ (simplify the gradient by defining $Z = \sum_{i=1}^3\exp(x_i)$).
		\ans{$\nabla f(x) = \mat{\exp(x_1)/Z\\\exp(x_2)/Z\\\exp(x_3)/Z\\}$ or $\nabla f(x) = \frac{1}{Z}\mat{\exp(x_1)\\\exp(x_2)\\\exp(x_3)}$.}
		\item $f(x) = a^Tx + b$ where $x \in \R^3$ and $a \in \R^3$ and $b \in \R$.
		\ans{$\nabla f(x) = \mat{a_1 \\a_2\\a_3}$ or  $\nabla f(x) = a$.}
		\item $f(x) = \half x^\top A x$ where $A=\left[ \begin{array}{cc}
			2 & -1 \\
			-1 & 2 \end{array} \right]$ and $x \in \mathbb{R}^2$.
		\ans{$\nabla f(x) = \mat{2x_1 - x_2\\-x_1 + 2x_2}$ or  $\nabla f(x) = Ax.$}
		\item $f(x) = \frac{1}{2}\norm{x}^2$ where $x \in \R^d$.
		\ans{$\nabla f(x) = \mat{x_1\\x_2\\\vdots\\x_d}$ or $\nabla f(x) = x$.}
	\end{enumerate}
	
	Hint: it is helpful to write out the linear algebra expressions in terms of summations.
	
	
	\subsection{Optimization}
	\blu{Find the following quantities.} You do not need to show your work. 
	You can/should use your results from parts \ref{sub.one.var} and \ref{sub.multi.var} as part of the process.
	
	\begin{enumerate}
		\item $\min \, 3x^2-2x+5$, or, in words, the minimum value of the function $f(x) = 3x^2 -2x + 5$ for $x \in \R$.
		\ans{$f'(x) = 6x - 2$. Setting this to zero and solving we get $x = 1/3$. The second derivative is $f''(x) = 6$ which is positive for all $x$ so this is a minimizer. Plugging this back in gives $1/3 - 2/3 + 5 = 4\frac{2}{3}$.}
		\item $\max \, x(1-x)$ for $x\in [0,1]$.
		\ans{$f'(x) = 1 - 2x$ and $f''(x) = -2$, so ignoring the constraints the function is maximized at $x=1/2$ with a value of $1/4$. But this satisfies the constraints so it's also the constrained maximum.}
		\item $\min \, x(1-x)$ for $x\in [0,1]$.
		\ans{We know that the function is curved downwards in both directions away from the maximum, and has no other stationary points, so the minimum has be at one of the end points of the interval, $x = 0$ or $x=1$. Plugging in either of these values gives $0$.}
		\item $\arg \max \, x(1-x)$ for $x\in[0,1]$. 
		\ans{As shows above, the maximum occurs at $x=1/2$.}
		\item $\min \, x_1^2 + \exp(x_2)$ where $x \in [0,1]^2$, or in other words $x_1\in [0,1]$ and $x_2\in [0,1]$.
		\ans{Since one term involves $x_1$ only and the other involves $x_2$ only, we can minimize them separately. They are minimized at $x_1=0$ and $x_2=0$. So the minimum is $f(x)=1$.}
		\item $\arg \min \, x_1^2 + \exp(x_2)$ where $x \in [0,1]^2$. 
		\ans{See explanation above. The minimum occurs at $x_1=0$ and $x_2=0$, or $x=\begin{pmatrix}0 \\ 0 \end{pmatrix}$.}
	\end{enumerate}
	
	Note: the notation $x\in [0,1]$ means ``$x$ is in the interval $[0,1]$'', or, also equivalently, $0 \leq x \leq 1$.
	
	Note: the notation ``$\max \, f(x)$'' means ``the value of $f(x)$ where $f(x)$ is maximized'', whereas ``$\arg \max \, f(x)$'' means ``the value of $x$ such that $f(x)$ is maximized''.
	Likewise for $\min$ and $\arg \min$. For example, the min of the function $f(x)=(x-1)^2$ is $0$ because the smallest possible value is $f(x)=0$, 
	whereas the arg min is $1$ because this smallest value occurs at $x=1$. The min is always a scalar but the $\arg \min$ is a value of $x$, so it's a vector 
	if $x$ is vector-valued.
	
	\subsection{Derivatives of code}
	
	\rubric{points:4}
	
	Your repository contains a file named \texttt{grads.py} which defines several Python functions that take in an input variable $x$, which we assume to be a 1-d array (in math terms, a vector).
	It also includes (blank) functions that return the corresponding gradients.
	For each function, \blu{write code that computes the gradient of the function} in Python.
	You should do this directly in \texttt{grads.py}; no need to make a fresh copy of the file. When finished, you can run \texttt{python main.py -q 3.4} to test out your code. \blu{Submit this code as a screenshot or using the \texttt{lstlisting} environment.}
	
	Hint: it's probably easiest to first understand on paper what the code is doing, then compute
	the gradient, and then translate this gradient back into code.
	
	Note: do not worry about the distinction between row vectors and column vectors here.
	For example, if the correct answer is a vector of length 5, we'll accept numpy arrays
	of shape \texttt{(5,)} (a 1-d array) or \texttt{(5,1)} (a column vector) or
	\texttt{(1,5)} (a row vector). In future assignments we will start to be more careful
	about this.
	
	Warning: Python uses whitespace instead of curly braces to delimit blocks of code.
	Some people use tabs and other people use spaces. My text editor (Atom) inserts 4 spaces (rather than tabs) when
	I press the Tab key, so the file \texttt{grads.py} is indented in this manner. If your text editor inserts tabs,
	Python will complain and you might get mysterious errors... this is one of the most annoying aspects
	of Python, especially when starting out. So, please be aware of this issue! And if in doubt you can just manually
	indent with 4 spaces, or convert everything to tabs. For more information
	see \url{https://www.youtube.com/watch?v=SsoOG6ZeyUI}.
	
	
	\section{Algorithms and Data Structures Review}
	
	\subsection{Trees}
	\rubric{points:2}
	
	\blu{Answer the following questions.} You do not need to show your work.
	
	\begin{enumerate}
		\item What is the minimum depth of a binary tree with 64 leaf nodes?
		\ans{$\log_2(64) = 6$.}
		\item What is the minimum depth of binary tree with 64 nodes (includes leaves and all other nodes)?
		\ans{Depth $5$ would have a maximum of $(2^0+2^1+2^2+2^3+2^4+2^5) = (1+2+4+8+16+32) = 2^{6}-1 = 63$, so this would also be $6$.}
	\end{enumerate}
	Note: we'll use the standard convention that the leaves are not included in the depth, so a tree with depth $1$ has 3 nodes with 2 leaves.
	
	\subsection{Common Runtimes}
	\rubric{points:4}
	
	\blu{Answer the following questions using big-$O$ notation} You do not need to show your work.
	\begin{enumerate}
		\item What is the cost of finding the largest number in an unsorted list of $n$ numbers?
		\ans{$O(n)$ by searching through the list and tracking the max.}
		\item What is the cost of finding the smallest element greater than 0 in a \emph{sorted} list with $n$ numbers.
		\ans{$O(\log n)$ with binary search.}
		\item What is the cost of finding the value associated with a key in a hash table with $n$ numbers? \\(Assume the values and keys are both scalars.)
		\ans{$O(1)$ using standard implementations.}
		\item What is the cost of computing the inner product $a^Tx$, where $a$ is $d \times 1$ and $x$ is $d \times 1$?
		\ans{$O(d)$ ($d$ multiplications and $(d-1)$ additions).}
		\item What is the cost of computing the quadratic form $x^TAx$ when $A$ is $d \times d$ and $x$ is $d \times 1$.
		\ans{$O(d^2)$. Computing $Ax$ is a matrix-vector product involving $d$ dot products, each one involving $d$ multiplications/additions, hence $d^2$. The secon computation, $x^T(Ax)$ is just the dot proudct between two vectors, which costs $O(d)$ and is negligible.}
	\end{enumerate}
	
	\subsection{Running times of code}
	\rubric{points:4}
	
	Your repository contains a file named \texttt{bigO.py}, which defines several functions
	that take an integer argument $N$. For each function, \blu{state the running time as a function of $N$, using big-O notation}.
	\ans{$\mathcal{O}(N), \mathcal{O}(N), \mathcal{O}(1), \mathcal{O}(N^2)$}
	
	
	\section{Data Exploration}
	
	
	Your repository contains the file \texttt{fluTrends.csv}, which contains estimates
	of the influenza-like illness percentage over 52 weeks on 2005-06 by Google Flu Trends.
	Your \texttt{main.py} loads this data for you and stores it in a pandas DataFrame \texttt{X},
	where each row corresponds to a week and each column
	corresponds to a different
	region. If desired, you can convert from a DataFrame to a raw numpy array with \texttt{X.values()}.
	
	\subsection{Summary Statistics}
	\rubric{points:2}
	
	\blu{Report the following statistics}:
	\enum{
		\item The minimum, maximum, mean, median, and mode of all values across the dataset.
		\item The $5\%$, $25\%$, $50\%$, $75\%$, and $95\%$ quantiles of all values across the dataset.
		\item The names of the regions with the highest and lowest means, and the highest and lowest variances.% \item The pairs of regions with the highest and lowest correlations.
	}
	In light of the above, \blu{is the mode a reliable estimate of the most ``common" value? Describe another way we could give a meaningful ``mode" measurement for this (continuous) data.} Note: the function \texttt{utils.mode()} will compute the mode value of an array for you.
	
	% Note: by ``lowest'' we mean most negative, not the closest to zero.
	
	\answer{
		\begin{enumerate}
			\item Minimum is 0.3520, maximum is 4.8620, mean is 1.3246, median is 1.1590, mode is 0.7700.
			\item The quantiles are 0.46, 0.72, 1.16, 1.81, and 2.62.
			\item The highest mean is \emph{WtdILI} and highest variance is \emph{Mtn}, the lowest man is \emph{Pac} and it also has the lowest variance.
			% \item The lowest correlation is between \emph{Mtn} and \emph{NE}, the highest is between \emph{ENCentral} and \emph{MidAtl}.
		\end{enumerate}
		See the code directory for code. The mode is not necessarily reliable because this is a
		continuous quantity. It just happened by chance that 4 values of 0.7700 are present
		in the data, but chance could have led to a very different value (or no values
		occurring more than once). One way to get a better estimate of the most
		``common'' value is to discretize, and then take the mode.
		For example if you discretize to 1 or 2 decimal places you would get a
		value of 0.5, if you discretize to integers you would get a mode of 1.
		Another approach could be to make a histogram, which also reveals that
		a mode somewhere in the range $[0.5,1]$ is reasonable.
	}
	
	
	
	\subsection{Data Visualization}
	\rubric{points:3}
	
	Consider the figure below.
	
	\fig{1}{../figs/visualize-unlabeled}
	
	The figure contains the following plots, in a shuffled order:
	\enum{
		\item A single histogram showing the distribution of \emph{each} column in $X$.
		\item A histogram showing the distribution of each the values in the matrix $X$.
		\item A boxplot grouping data by weeks, showing the distribution across regions for each week.
		\item A plot showing the illness percentages over time.
		\item A scatterplot between the two regions with highest correlation.
		\item A scatterplot between the two regions with lowest correlation.
	}
	
	\blu{Match the plots (labeled A-F) with the descriptions above (labeled 1-6), with an extremely brief (a few words is fine) explanation for each decision.}
	
	\answer{1:D, 2:C, 3:B, 4:A, 5:F 6:E}
	
	
	
	
	\section{Decision Trees}
	
	If you run \texttt{python main.py -q 6}, it will load a dataset containing longitude 
	and latitude data for 400 cities in the US, along with a class label indicating
	whether they were a ``red" state or a ``blue" state in the 2012 
	election.\footnote{The cities data was sampled from \url{http://simplemaps.com/static/demos/resources/us-cities/cities.csv}. The election information was collected from Wikipedia.}
	Specifically, the first column of the variable $X$ contains the 
	longitude and the second variable contains the latitude,
	while the variable $y$ is set to $0$ for blue states and $1$ for red states.
	After it loads the data, it plots the data and then fits two simple 
	classifiers: a classifier that always predicts the
	most common label ($0$ in this case) and a decision stump
	that discretizes the features (by rounding to the nearest integer)
	and then finds the best equality-based rule (i.e., check
	if a feature is equal to some value).
	It reports the training error with these two classifiers, then plots the decision areas made by the decision stump.
	The plot is shown below:
	
	\centerfig{0.7}{../figs/q6_decisionBoundary}
	
	As you can see, it is just checking whether the latitude equals 35 and, if so, predicting red (Republican).
	This is not a very good classifier. 
	
	\subsection{Splitting rule}
	\rubric{points:1}
	
	Is there a particular type of features for which it makes sense to use an equality-based splitting rule rather than the threshold-based splits we discussed in class?
	
	\answer{Categorical features.}
	
	\subsection{Decision Stump Implementation}
	\rubric{points:3}
	
	The file \texttt{decision\string_stump.py} contains the class \texttt{DecisionStumpEquality} which 
	finds the best decision stump using the equality rule and then makes predictions using that
	rule. Instead of discretizing the data and using a rule based on testing an equality for 
	a single feature, we want to check whether a feature is above or below a threshold and 
	split the data accordingly (this is a more sane approach, which we discussed in class). 
	\blu{Create a \texttt{DecisionStumpErrorRate} class to do this, and report the updated error you 
		obtain by using inequalities instead of discretizing and testing equality. 
		Submit your class definition code as a screenshot or using the \texttt{lstlisting} environment.
		Also submit the generated figure of the classification boundary.}
	
	Hint: you may want to start by copy/pasting the contents \texttt{DecisionStumpEquality} and then make modifications from there. %You can make modifications from there. % But you are asked to keep them separate rather than directly modifying the provided code so that you can keep both versions in tact and compare them.
	% Note: please keep the same variable names, as subsequent parts of this assignment rely on this!
	
	\answer{
		% The code should look something like this:
		% \lstinputlisting[language=Matlab]{../code/decisionStump.m}
		See code.
		The improvements reduce the error from $0.41$ to $0.25$, by splitting the country into 
		a northern (blue) part and a southern (red) part:\\
		\centerfig{.7}{../figs/q6_2_decisionBoundary}
	}
	
	
	
	\subsection{Decision Stump Info Gain Implementation}
	\rubric{points:3}
	
	In \texttt{decision\string_stump.py}, \blu{create a \texttt{DecisionStumpInfoGain} class that 
		fits using the information gain criterion discussed in lecture. 
		Report the updated error you obtain.
		Submit your class definition code as a screenshot or using the \texttt{lstlisting} environment.
		Submit the classification boundary figure.}
	
	Notice how the error rate changed. Are you surprised? If so, hang on until the end of this question!
	
	Note: even though this data set only has 2 classes (red and blue), your implementation should work 
	for any number of classes, just like \texttt{DecisionStumpEquality} and \texttt{DecisionStumpErrorRate}.
	
	Hint: take a look at the documentation for \texttt{np.bincount}, at \\
	\url{https://docs.scipy.org/doc/numpy/reference/generated/numpy.bincount.html}. 
	The \texttt{minlength} argument comes in handy here to deal with a tricky corner case:
	when you consider a split, you might not have any cases of a certain class, like class 1,
	going to one side of the split. Thus, when you call \texttt{np.bincount}, you'll get
	a shorter array by default, which is not what you want. Setting \texttt{minlength} to the 
	number of classes solves this problem.
	
	
	\answer{
		Updated error: 0.325. See code.
		\centerfig{.7}{../figs/q6_3_decisionBoundary}
	}
	
	
	\subsection{Hard-coded Decision Trees}
	\rubric{points:2}
	
	Once your \texttt{DecisionStumpInfoGain} class is finished, running \texttt{python main.py -q 6.4} will fit
	a decision tree of depth~2 to the same dataset (which results in a lower training error).
	Look at how the decision tree is stored and how the (recursive) \texttt{predict} function works.
	\blu{Using the splits from the fitted depth-2 decision tree, write a hard-coded version of the \texttt{predict}
		function that classifies one example using simple if/else statements
		(see the Decision Trees lecture). Submit this code as a plain text, as a screenshot or using the \texttt{lstlisting} environment.}
	
	Note: this code should implement the specific, fixed decision tree
	which was learned after calling \texttt{fit} on this particular data set. It does not need to be a learnable model.
	You should just hard-code the split values directly into the code.
	Only the \texttt{predict} function is needed.
	
	Hint: if you plot the decision boundary you can do a visual sanity check to see if your code is consistent with the plot.
	
	
	
	
	
	\subsection{Decision Tree Training Error}
	\rubric{points:2}
	
	Running \texttt{python main.py -q 6.5} fits decision trees of different depths using the following different implementations: 
	\enum{
		\item A decision tree using \texttt{DecisionStumpErrorRate}
		\item A decision tree using \texttt{DecisionStumpInfoGain}
		\item The \texttt{DecisionTreeClassifier} from the popular Python ML library \emph{scikit-learn}
	}
	
	% (3) and (3) above use a more sophisticated splitting criterion called the information gain, instead of just the classification accuracy.
	Run the code and look at the figure.
	\blu{Describe what you observe. Can you explain the results?} Why is approach (1) so disappointing? Also, \blu{submit a classification boundary plot of the model with the lowest training error}.
	
	Note: we set the \verb|random_state| because sklearn's \texttt{DecisionTreeClassifier} is non-deterministic. This is probably
	because it breaks ties randomly.
	
	Note: the code also prints out the amount of time spent. You'll notice that sklearn's implementation is substantially faster. This is because
	our implementation is based on the $O(n^2d)$ decision stump learning algorithm and sklearn's implementation presumably uses the faster $O(nd\log n)$
	decision stump learning algorithm that we discussed in lecture.
	
	\answer{
		% The decision tree implements this program:\\
		% \lstinputlisting[language=Matlab]{../code/decisionTreeProgram.m}
		The training error plateaus in our implementation after a depth of 4, whereas for the sklearn implementation the training error goes all the way to zero.
		The training stops decreasing after depth 4 because there is no rule to split
		any of the leaves that increases the classification accuracy:
		just assigning the majority label gives the same or better accuracy than any split (e.g., because both sides of the split have the same mode of the labels).
		Here's the plot:\\
		\centerfig{0.7}{../figs/q6_5_decisionBoundary.pdf}
	}
	
	\subsection{Comparing implementations}
	\rubric{points:2}
	
	In the previous section you compared different implementations of a machine learning algorithm. Let's say that two
	approaches produce the exact same curve of classification error rate vs. tree depth. Does this conclusively demonstrate
	that the two implementations are the same? If so, why? If not, what other experiment might you perform to build confidence
	that the implementations are probably equivalent?
	
	\answer{
		No, they aren't necessarily the same just because they get the same error rate. They might be classifying examples
		differently but just happening to get the same number of errors. One simple test is to make sure they are actually
		making the same predictions for all the inputs in the data set. You could also try this for randomly generated inputs -
		they don't have to come form the data set, necessarily. 
	}
	
	\subsection{Cost of Fitting Decision Trees}
	\rubric{points:3}
	
	In class, we discussed how in general the decision stump minimizing the classification error can be found in $O(nd\log n)$ time.
	Using the greedy recursive splitting procedure, \blu{what is the total cost of fitting a decision tree of depth $m$ in terms of $n$, $d$, and $m$?}
	
	Hint: even thought there could be $(2^m-1)$ decision stumps, keep in mind not every stump will need to go through every example. Note also that we stop growing the decision tree if a node has no examples, so we may not even need to do anything for many of the $(2^m-1)$ decision stumps.
	
	\answer{The number of stumps that we need to fit for a decision tree of depth $m$ will be $(2^m-1)$,
		so a naive analysis would indicate that we need $O(2^mnd\log n)$ time.
		However, at each depth we have split the $n$ examples across the decision stumps.
		This means that each depth will only need to look at $n$ examples,
		and the cost for each layer is still $O(nd\log n)$.
		This gives a total cost $O(mnd\log n)$ to go through all $m$ layers.
		\footnote{It's actually possible to go a little faster if you notice that you only need to sort once,
			and with some bookkeeping you maintain the examples in sorted order as you split them. This would reduce the cost slightly to $O(nd\log n + mnd)$.}}
	
	
\end{document}